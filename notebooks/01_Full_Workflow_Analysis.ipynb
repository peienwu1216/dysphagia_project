{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def read_h5_to_dataframe(file_path: str, group: str | None = \"rf\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Read datasets from an HDF5 file into a flat pandas DataFrame.\n",
        "\n",
        "    - If `group` is provided and exists, only read from that group; otherwise read from file root.\n",
        "    - 1D datasets become single columns.\n",
        "    - 2D datasets are expanded to multiple columns with suffixes _c0.._c{n-1}.\n",
        "    - Higher dimensional datasets are flattened along the last axis.\n",
        "\n",
        "    Returns empty DataFrame if nothing suitable is found.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "    def _collect_from_group(h5group: h5py.Group) -> dict[str, np.ndarray]:\n",
        "        columns: dict[str, np.ndarray] = {}\n",
        "\n",
        "        def visit(name, obj):\n",
        "            # Only datasets; skip groups\n",
        "            if not isinstance(obj, h5py.Dataset):\n",
        "                return\n",
        "            ds = obj[...]\n",
        "            key_base = name.replace(\"/\", \"_\")\n",
        "\n",
        "            # Handle complex dtype by splitting into real/imag if needed\n",
        "            if np.iscomplexobj(ds):\n",
        "                data_real = np.asarray(np.real(ds))\n",
        "                data_imag = np.asarray(np.imag(ds))\n",
        "                # Normalize shapes below\n",
        "                _assign_dataset(columns, f\"{key_base}_real\", data_real)\n",
        "                _assign_dataset(columns, f\"{key_base}_imag\", data_imag)\n",
        "            else:\n",
        "                data = np.asarray(ds)\n",
        "                _assign_dataset(columns, key_base, data)\n",
        "\n",
        "        h5group.visititems(visit)\n",
        "        return columns\n",
        "\n",
        "    def _assign_dataset(columns: dict[str, np.ndarray], key_base: str, data: np.ndarray) -> None:\n",
        "        # Squeeze only redundant singleton dimensions (not flattening meaningful axes)\n",
        "        data = np.squeeze(data)\n",
        "        if data.ndim == 0:\n",
        "            # Scalar -> make it length-1 column\n",
        "            columns[key_base] = np.array([data])\n",
        "        elif data.ndim == 1:\n",
        "            columns[key_base] = data\n",
        "        elif data.ndim == 2:\n",
        "            # Expand each column\n",
        "            num_cols = data.shape[1]\n",
        "            for col_idx in range(num_cols):\n",
        "                columns[f\"{key_base}_c{col_idx}\"] = data[:, col_idx]\n",
        "        else:\n",
        "            # For >=3D, flatten all but the first dimension\n",
        "            first_dim = data.shape[0]\n",
        "            reshaped = data.reshape(first_dim, -1)\n",
        "            for col_idx in range(reshaped.shape[1]):\n",
        "                columns[f\"{key_base}_c{col_idx}\"] = reshaped[:, col_idx]\n",
        "\n",
        "    with h5py.File(file_path, \"r\") as hf:\n",
        "        if group is not None and group in hf and isinstance(hf[group], h5py.Group):\n",
        "            columns = _collect_from_group(hf[group])\n",
        "        else:\n",
        "            columns = _collect_from_group(hf)\n",
        "\n",
        "    # Align columns by length (use the max first-dimension length and drop mismatched)\n",
        "    if not columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    lengths = {k: (len(v) if v.ndim == 1 else v.shape[0]) for k, v in columns.items()}\n",
        "    target_len = max(lengths.values())\n",
        "\n",
        "    aligned = {}\n",
        "    for k, v in columns.items():\n",
        "        v = np.asarray(v)\n",
        "        if v.ndim == 1 and len(v) == target_len:\n",
        "            aligned[k] = v\n",
        "        elif v.ndim == 2 and v.shape[0] == target_len:\n",
        "            # Unlikely here since we expanded earlier, but keep for safety\n",
        "            for j in range(v.shape[1]):\n",
        "                aligned[f\"{k}_c{j}\"] = v[:, j]\n",
        "        elif v.ndim == 1 and len(v) < target_len:\n",
        "            # Pad with NaN to match target length\n",
        "            pad = np.full(target_len, np.nan)\n",
        "            pad[: len(v)] = v\n",
        "            aligned[k] = pad\n",
        "        else:\n",
        "            # Skip columns with incompatible length\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(aligned)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ File not found: c:\\Users\\USER\\Documents\\GitHub\\dysphagia_project\\data\\raw\\S01_tiptoe_01.h5\n"
          ]
        }
      ],
      "source": [
        "# Load a single .h5 file into a tidy DataFrame\n",
        "file_path = os.path.abspath(os.path.join('..', 'data', 'raw', 'S01_tiptoe_01.h5'))\n",
        "try:\n",
        "    df_rf = read_h5_to_dataframe(file_path, group='rf')\n",
        "    if df_rf.empty:\n",
        "        print('Loaded file but found no datasets to tabularize. Falling back to file root...')\n",
        "        df_rf = read_h5_to_dataframe(file_path, group=None)\n",
        "\n",
        "    print('âœ… Data loaded')\n",
        "    print('Shape:', df_rf.shape)\n",
        "    print('Columns sample:', list(df_rf.columns)[:10])\n",
        "    display(df_rf.head())\n",
        "except FileNotFoundError as e:\n",
        "    print(f'âŒ {e}')\n",
        "except Exception as e:\n",
        "    print(f'âŒ Failed to load H5 file: {e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# åå’½éšœç¤™æª¢æ¸¬ - å®Œæ•´ä¿¡è™Ÿè™•ç†æµç¨‹\n",
        "\n",
        "æœ¬notebookå¯¦ç¾äº†å¾åŸå§‹RFä¿¡è™Ÿåˆ°ç‰¹å¾µæå–çš„å®Œæ•´è™•ç†æµç¨‹ï¼Œç”¨æ–¼åå’½éšœç¤™æª¢æ¸¬ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å€å¡Š 1ï¼šå°ˆæ¡ˆè¨­å®šèˆ‡æ•¸æ“šè®€å–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================= #\n",
        "# å€å¡Š 1: å°ˆæ¡ˆè¨­å®šèˆ‡æ•¸æ“šè®€å–\n",
        "# ================================================================= #\n",
        "\n",
        "# --- æ­¥é©Ÿ 0: è¨­å®š Python æœå°‹è·¯å¾‘ ---\n",
        "import sys\n",
        "import os\n",
        "# å°‡åŒ…å« nearpy å¥—ä»¶çš„è³‡æ–™å¤¾åŠ å…¥åˆ° Python çš„æœå°‹è·¯å¾‘ä¸­\n",
        "sys.path.insert(0, os.path.abspath('../nearpy'))\n",
        "\n",
        "# --- æ­¥é©Ÿ 1: åŒ¯å…¥æ‰€æœ‰éœ€è¦çš„å·¥å…· ---\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# å¾ nearpy å‡½å¼åº«ä¸­åŒ¯å…¥è¨Šè™Ÿè™•ç†èˆ‡ç¹ªåœ–æ¨¡çµ„\n",
        "from nearpy.preprocess import filters, segment\n",
        "from nearpy.plots import timedomain\n",
        "from nearpy.features import temporal, spectral\n",
        "from nearpy.ai.utils import make_dataset\n",
        "\n",
        "# è®“åœ–å½¢åœ¨ notebook ä¸­ç›´æ¥é¡¯ç¤º\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "print(\"âœ… æ‰€æœ‰å·¥å…·å·²æˆåŠŸåŒ¯å…¥ï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- æ­¥é©Ÿ 2: å»ºç«‹è®€å– .h5 æª”æ¡ˆçš„è¼”åŠ©å‡½å¼ ---\n",
        "def read_h5_data(file_path):\n",
        "    \"\"\"\n",
        "    å¾ HDF5 (.h5) æª”æ¡ˆä¸­è®€å– RF æ•¸æ“šï¼Œä¸¦è‡ªå‹•å°‡å¤šç¶­è³‡æ–™å±•é–‹ç‚º 1D æ¬„ä½ã€‚\n",
        "    - æœƒéè¿´èµ°è¨ªæ‰€æœ‰ groupï¼Œæ“·å– dataset\n",
        "    - ä»¥ã€Œæ™‚é–“è»¸ã€ç‚ºç¬¬ä¸€ç¶­ï¼Œå…¶ä»–ç¶­åº¦å±•å¹³æˆå¤šå€‹é€šé“æ¬„ä½\n",
        "    - å°æ–¼ä¸åŒ dataset é•·åº¦ä¸ä¸€è‡´ï¼Œæœƒå°é½Šåˆ°å…±åŒçš„æœ€çŸ­é•·åº¦\n",
        "    - å° 1D ä½†å…ƒç´ ç‚ºé™£åˆ— (object/vlen) çš„è³‡æ–™ï¼Œè‡ªå‹•å˜—è©¦å †ç–Šæˆ 2Dï¼›è‹¥é•·åº¦ä¸ä¸€è‡´å‰‡è·³é\n",
        "    \"\"\"\n",
        "    def iter_datasets(h, prefix=\"\"):\n",
        "        for key in h.keys():\n",
        "            obj = h[key]\n",
        "            name = f\"{prefix}/{key}\" if prefix else key\n",
        "            if isinstance(obj, h5py.Group):\n",
        "                yield from iter_datasets(obj, name)\n",
        "            elif isinstance(obj, h5py.Dataset):\n",
        "                yield name.lstrip('/'), obj[()]\n",
        "\n",
        "    def expand_object_vector_if_possible(vec):\n",
        "        \"\"\"å°‡ dtype=object ä¸”å…ƒç´ ç‚ºç­‰é•·å‘é‡çš„ 1D é™£åˆ—å †ç–Šç‚º 2D (T, D)ã€‚\n",
        "        è‹¥å…ƒç´ ä¸æ˜¯ç­‰é•·ï¼Œå›å‚³ None ä»£è¡¨ç„¡æ³•è™•ç†ã€‚\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if vec.ndim != 1:\n",
        "                return None\n",
        "            if not (np.issubdtype(vec.dtype, np.object_) or vec.dtype == object):\n",
        "                return None\n",
        "            # å…ƒç´ é ˆç‚º array/list ä¸”ç­‰é•·\n",
        "            sample = next((v for v in vec if isinstance(v, (np.ndarray, list, tuple))), None)\n",
        "            if sample is None:\n",
        "                return None\n",
        "            target_len = len(sample)\n",
        "            if target_len == 0:\n",
        "                return None\n",
        "            for v in vec:\n",
        "                if not isinstance(v, (np.ndarray, list, tuple)) or len(v) != target_len:\n",
        "                    return None\n",
        "            stacked = np.stack([np.asarray(v).reshape(-1) for v in vec], axis=0)\n",
        "            return stacked  # å½¢ç‹€ (T, D)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        with h5py.File(file_path, 'r') as hf:\n",
        "            collected = []\n",
        "            # è‹¥å­˜åœ¨ 'rf' groupï¼Œå„ªå…ˆä½¿ç”¨ï¼›å¦å‰‡éæ­·æ•´å€‹æª”æ¡ˆ\n",
        "            root = hf['rf'] if 'rf' in hf else hf\n",
        "            for name, arr in iter_datasets(root, prefix='' if root is hf else 'rf'):\n",
        "                if np.isscalar(arr):\n",
        "                    continue  # è·³éç´”é‡\n",
        "                arr = np.asarray(arr)\n",
        "                if arr.ndim == 0:\n",
        "                    continue\n",
        "\n",
        "                # è™•ç† 1D object/vlenï¼šè‹¥æ¯å€‹å…ƒç´ æ˜¯ç­‰é•·å‘é‡ï¼Œå †ç–Šç‚º (T, D)\n",
        "                if arr.ndim == 1 and (np.issubdtype(arr.dtype, np.object_) or arr.dtype == object):\n",
        "                    expanded = expand_object_vector_if_possible(arr)\n",
        "                    if expanded is not None:\n",
        "                        arr = expanded\n",
        "                    else:\n",
        "                        # ç„¡æ³•å¯é å±•é–‹ï¼Œè·³éä»¥é¿å… DataFrame å»ºç«‹å¤±æ•—\n",
        "                        continue\n",
        "\n",
        "                # å°‹æ‰¾æœ€å¯èƒ½çš„æ™‚é–“è»¸ï¼šé¸æ“‡é•·åº¦æœ€å¤§çš„è»¸ä½œç‚ºæ™‚é–“\n",
        "                time_axis = int(np.argmax(arr.shape))\n",
        "                if time_axis != 0:\n",
        "                    arr = np.moveaxis(arr, time_axis, 0)\n",
        "                collected.append((name, arr))\n",
        "\n",
        "            if len(collected) == 0:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            # å°é½Šåˆ°å…±åŒçš„æœ€çŸ­æ™‚é–“é•·åº¦\n",
        "            valid_lengths = [a.shape[0] for _, a in collected if a.ndim >= 1 and a.shape[0] > 0]\n",
        "            if len(valid_lengths) == 0:\n",
        "                return pd.DataFrame()\n",
        "            min_len = min(valid_lengths)\n",
        "\n",
        "            column_dict = {}\n",
        "            for name, arr in collected:\n",
        "                if arr.shape[0] < 1:\n",
        "                    continue\n",
        "                arr = arr[:min_len]\n",
        "                if arr.ndim == 1:\n",
        "                    # ç¢ºä¿æ˜¯ 1D é€£çºŒé™£åˆ—\n",
        "                    column_dict[name] = np.asarray(arr).reshape(-1)\n",
        "                else:\n",
        "                    flat = arr.reshape(arr.shape[0], -1)\n",
        "                    for j in range(flat.shape[1]):\n",
        "                        column_dict[f\"{name}_{j}\"] = flat[:, j]\n",
        "\n",
        "            # åƒ…ä¿ç•™çœŸæ­£ 1D çš„æ¬„ä½ï¼Œé¿å… pandas å ±éŒ¯\n",
        "            safe_column_dict = {}\n",
        "            for k, v in column_dict.items():\n",
        "                vv = np.asarray(v)\n",
        "                if vv.ndim == 1 and len(vv) == min_len:\n",
        "                    safe_column_dict[k] = vv\n",
        "            if len(safe_column_dict) == 0:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df = pd.DataFrame(safe_column_dict)\n",
        "            # å»é™¤å…¨ç‚º NaN æˆ– å¸¸æ•¸çš„æ¬„ä½ï¼ˆè‹¥æœ‰ï¼‰\n",
        "            if df.shape[1] > 0:\n",
        "                df = df.loc[:, df.apply(lambda c: not (c.isna().all() or (np.nanmax(c) == np.nanmin(c))), axis=0)]\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        print(f\"!!! è®€å–æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e} !!!\")\n",
        "        return None\n",
        "\n",
        "# --- æ­¥é©Ÿ 3: è¨­å®šæª”æ¡ˆè·¯å¾‘ä¸¦è®€å–æ•¸æ“š ---\n",
        "# å‡è¨­æˆ‘å€‘åˆ†æå—è©¦è€… S01 çš„æ•¸æ“š\n",
        "file_path_s01 = '../data/raw/S01_tiptoe_01.h5'\n",
        "rf_data = read_h5_data(file_path_s01)\n",
        "\n",
        "if rf_data is not None and not rf_data.empty:\n",
        "    print(f\"âœ… æˆåŠŸè®€å–æ•¸æ“š: {file_path_s01}\")\n",
        "    print(\"æ•¸æ“šå½¢ç‹€:\", rf_data.shape)\n",
        "    print(\"æ•¸æ“šåŒ…å«çš„é€šé“ï¼ˆå‰10ï¼‰ï¼š\", rf_data.columns.tolist()[:10])\n",
        "    print(\"æ•¸æ“šå‰5è¡Œ:\")\n",
        "    print(rf_data.head())\n",
        "else:\n",
        "    print(\"âŒ æ•¸æ“šè®€å–å¤±æ•—ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘èˆ‡ .h5 çµæ§‹ï¼ˆä¸å»ºç«‹æ¨¡æ“¬æ•¸æ“šï¼‰ã€‚\")\n",
        "    # å¯è¦–åŒ–æª”æ¡ˆæª”æ¡ˆçµæ§‹è¼”åŠ©ï¼ˆå¿…è¦æ™‚æ‰‹å‹•åŸ·è¡Œï¼‰\n",
        "    # with h5py.File(file_path_s01, 'r') as hf:\n",
        "    #     hf.visititems(lambda name, obj: print(name, type(obj)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å€å¡Š 2ï¼šå‘ˆç¾åŸå§‹è¨Šè™Ÿ (Raw Data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================= #\n",
        "# å€å¡Š 2: å‘ˆç¾åŸå§‹è¨Šè™Ÿ (Raw Data)\n",
        "# ================================================================= #\n",
        "\n",
        "if rf_data is not None and not rf_data.empty:\n",
        "    cols = list(rf_data.columns)\n",
        "\n",
        "    # å˜—è©¦å¾ H5 å±¬æ€§è‡ªå‹•åµæ¸¬å–æ¨£ç‡ï¼›è‹¥å¤±æ•—å‰‡å›é€€åˆ° 100 Hz\n",
        "    def _infer_fs_from_h5(file_path):\n",
        "        cand_keys = {'fs','Fs','FS','sampling_rate','sample_rate','samplingRate','sampling_frequency','SamplingFrequency'}\n",
        "        try:\n",
        "            with h5py.File(file_path, 'r') as hf:\n",
        "                root = hf['rf'] if 'rf' in hf else hf\n",
        "                def check_attrs(obj):\n",
        "                    for k, v in obj.attrs.items():\n",
        "                        if str(k) in cand_keys and np.isscalar(v):\n",
        "                            try:\n",
        "                                return float(v)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                    return None\n",
        "                fs = check_attrs(root)\n",
        "                if fs is not None:\n",
        "                    return fs\n",
        "                found = None\n",
        "                def visitor(name, obj):\n",
        "                    nonlocal found\n",
        "                    if found is not None:\n",
        "                        return\n",
        "                    val = check_attrs(obj)\n",
        "                    if val is not None:\n",
        "                        found = val\n",
        "                root.visititems(visitor)\n",
        "                return found\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    fs_raw = _infer_fs_from_h5(file_path_s01) or 100.0\n",
        "    print(f\"ä½¿ç”¨å–æ¨£ç‡ fs = {fs_raw:.3f} Hz é€²è¡Œç¹ªåœ–ï¼ˆè‹¥ä¸æ­£ç¢ºå¯æ–¼ä¸Šæ–¹è®Šæ•¸ä¿®æ”¹ï¼‰\")\n",
        "\n",
        "    # ç•¶æ¬„ä½åç¨±æ²’æœ‰æ¨™ç¤º mag/phase æ™‚ï¼Œé€™æ‰¹è³‡æ–™çœ‹èµ·ä¾†åƒ 8 é€šé“äº¤éŒ¯ç‚º 4 çµ„ (Amp, Phase)\n",
        "    # ä¾åºå°‡ (data_0,data_1), (data_2,data_3) ... é…å°æˆ 4 çµ„ï¼›è‹¥å¯¦éš›æœ‰æ¨™ç±¤å‰‡å„ªå…ˆä½¿ç”¨æ¨™ç±¤\n",
        "    pairs = []\n",
        "    used = set()\n",
        "\n",
        "    # å…ˆç”¨åç¨±åŒ…å« mag/phase çš„è‡ªå‹•é…å°\n",
        "    mag_like = [c for c in cols if 'mag' in c.lower() or 'amp' in c.lower()]\n",
        "    phs_like = [c for c in cols if 'phase' in c.lower() or 'phs' in c.lower()]\n",
        "    for m in mag_like:\n",
        "        if len(phs_like) == 0:\n",
        "            break\n",
        "        m_idx = cols.index(m)\n",
        "        cand = sorted([(abs(cols.index(p) - m_idx), p) for p in phs_like if p not in used])\n",
        "        if len(cand) > 0:\n",
        "            p = cand[0][1]\n",
        "            pairs.append((m, p))\n",
        "            used.add(m); used.add(p)\n",
        "\n",
        "    # è‹¥ä»æœªæˆå°ï¼Œæ¡ç”¨å¶æ•¸-å¥‡æ•¸é€šé“æ¨æ¸¬è¦å‰‡\n",
        "    remaining = [c for c in cols if c not in used]\n",
        "    if len(remaining) >= 2 and all(c.startswith('data_') for c in remaining):\n",
        "        for i in range(0, len(remaining) - 1, 2):\n",
        "            pairs.append((remaining[i], remaining[i + 1]))\n",
        "\n",
        "    if len(pairs) == 0:\n",
        "        print('âŒ ç„¡æ³•æ¨æ¸¬ Amp/Phase é…å°ï¼Œè«‹æª¢æŸ¥æ¬„ä½åç¨±ã€‚å¯ç”¨é€šé“:', cols)\n",
        "    else:\n",
        "        # è¼•åº¦ä¸‹æ¡æ¨£ï¼Œé¿å…ç¹ªåœ–å¤ªå¯†ï¼›åŒæ™‚è½‰ç‚ºç§’\n",
        "        ds = 10  # æ¯ 10 é»å– 1 é»\n",
        "        t_idx = np.arange(0, len(rf_data), ds)\n",
        "        t_sec = t_idx / fs_raw\n",
        "\n",
        "        n_rows = len(pairs)\n",
        "        fig, axes = plt.subplots(n_rows, 1, figsize=(15, 2.2 * n_rows), sharex=True)\n",
        "        if n_rows == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        raw_mag_signal = None  # ä¿å­˜ç¬¬ä¸€çµ„çš„æŒ¯å¹…ä¾›å¾ŒçºŒæ¿¾æ³¢å–®å…ƒä½¿ç”¨\n",
        "\n",
        "        for i, (mag_ch, phs_ch) in enumerate(pairs):\n",
        "            ax = axes[i]\n",
        "            # ä¸­å¿ƒåŒ–è®“è¶¨å‹¢æ›´æ¸…æ¥šï¼ˆä¸æ”¹è®Šç›¸å°å½¢ç‹€ï¼‰\n",
        "            mag = rf_data[mag_ch].values\n",
        "            phs = rf_data[phs_ch].values\n",
        "            mag_c = mag - np.nanmedian(mag)\n",
        "            phs_c = phs - np.nanmedian(phs)\n",
        "\n",
        "            if raw_mag_signal is None:\n",
        "                raw_mag_signal = mag  # ä¿ç•™å®Œæ•´å–æ¨£ç‡ç‰ˆæœ¬\n",
        "                raw_phase_signal = phs\n",
        "                fs = fs_raw  # ä¹ŸåŒæ­¥æ›´æ–°å¾ŒçºŒå€å¡Šä½¿ç”¨çš„ fsï¼ˆè‹¥å…ˆå‰æœªå®šç¾©ï¼‰\n",
        "\n",
        "            ax.plot(t_sec, mag_c[t_idx], color='tab:blue', lw=0.8, alpha=0.9, label=f'Amp ({mag_ch})')\n",
        "            ax2 = ax.twinx()\n",
        "            ax2.plot(t_sec, phs_c[t_idx], color='k', lw=0.8, alpha=0.7, label=f'Phs ({phs_ch})')\n",
        "            ax.set_ylabel('Amp')\n",
        "            ax2.set_ylabel('Phs (rad)')\n",
        "            ax.set_title(f'Pair {i + 1}  {mag_ch} / {phs_ch}')\n",
        "            lines, labels = ax.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax.legend(lines + lines2, labels + labels2, loc='upper right', fontsize=8)\n",
        "\n",
        "        axes[-1].set_xlabel('æ™‚é–“ (s)')\n",
        "        fig.suptitle(f'åŸå§‹è¨Šè™Ÿï¼ˆæ¨æ¸¬çš„ Amp/Phase é…å°ï¼‰  |  fs={fs_raw:.3f} Hz', y=1.02)\n",
        "\n",
        "        # é¡¯ç¤ºæ¸…æ¥šçš„æ™‚é–“åˆ»åº¦ï¼šä¸»åˆ»åº¦ 0.5sã€æ¬¡åˆ»åº¦ 0.1s\n",
        "        from matplotlib.ticker import MultipleLocator, AutoMinorLocator\n",
        "        for ax in axes:\n",
        "            ax.xaxis.set_major_locator(MultipleLocator(0.5))\n",
        "            ax.xaxis.set_minor_locator(MultipleLocator(0.1))\n",
        "            ax.grid(True, which='major', alpha=0.35)\n",
        "            ax.grid(True, which='minor', alpha=0.15)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"âŒ æ²’æœ‰å¯ç”¨çš„æ•¸æ“š\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å€å¡Š 3ï¼šè¨Šè™Ÿæ¿¾æ³¢ (Filtering)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================= #\n",
        "# å€å¡Š 3: è¨Šè™Ÿæ¿¾æ³¢ (Filtering)\n",
        "# ================================================================= #\n",
        "\n",
        "if 'raw_mag_signal' in locals():\n",
        "    print(\"æ­£åœ¨å°åŸå§‹æŒ¯å¹…è¨Šè™Ÿé€²è¡Œæ¿¾æ³¢...\")\n",
        "    \n",
        "    # --- åƒæ•¸è¨­å®š ---\n",
        "    # å‡è¨­æˆ‘å€‘çš„æ•¸æ“šå–æ¨£ç‡ç‚º 100 Hz\n",
        "    fs = 100 \n",
        "    \n",
        "    # --- è¨Šè™Ÿè™•ç†æ­¥é©Ÿ ---\n",
        "    # 1. å»é™¤åŸºç·šæ¼‚ç§» (ä½¿ç”¨3æ¬¡å¤šé …å¼å»è¶¨å‹¢)\n",
        "    from nearpy.preprocess.filters import detrend\n",
        "    mag_detrend = detrend(raw_mag_signal, deg=3)\n",
        "    \n",
        "    # 2. å¥—ç”¨å°ˆç‚ºè‚Œè‚‰/æ‰‹å‹¢é‹å‹•è¨­è¨ˆçš„å¸¶é€šæ¿¾æ³¢å™¨\n",
        "    from nearpy.preprocess.filters import get_gesture_filter\n",
        "    from scipy.signal import filtfilt\n",
        "    \n",
        "    # ç²å–æ¿¾æ³¢å™¨ä¿‚æ•¸\n",
        "    filter_taps = get_gesture_filter(f_s=15, fs=fs, visualize=False)\n",
        "    \n",
        "    # æ‡‰ç”¨æ¿¾æ³¢å™¨\n",
        "    mag_filtered = filtfilt(filter_taps, 1, mag_detrend)\n",
        "\n",
        "    print(\"âœ… æ¿¾æ³¢å®Œæˆï¼\")\n",
        "    print(\"æ­£åœ¨ç¹ªè£½æ¿¾æ³¢å¾Œçµæœ (å°æ‡‰ filtered.png)...\")\n",
        "\n",
        "    # ç¹ªåœ–æ¯”è¼ƒåŸå§‹è¨Šè™Ÿèˆ‡æ¿¾æ³¢å¾Œè¨Šè™Ÿ\n",
        "    fig, ax = plt.subplots(figsize=(15, 8))\n",
        "    \n",
        "    # åŸå§‹è¨Šè™Ÿ\n",
        "    ax.plot(raw_mag_signal, label='Raw Amplitude', color='tab:blue', alpha=0.6)\n",
        "    \n",
        "    # å»è¶¨å‹¢å¾Œ\n",
        "    ax.plot(mag_detrend, label='Detrended', color='tab:orange', alpha=0.7)\n",
        "    \n",
        "    # æ¿¾æ³¢å¾Œ\n",
        "    ax.plot(mag_filtered, label='Filtered Amplitude', color='tab:red', linewidth=1.5)\n",
        "    \n",
        "    ax.set_xlabel(\"å–æ¨£é» (Sampling Points)\")\n",
        "    ax.set_ylabel(\"æŒ¯å¹… (Amplitude)\")\n",
        "    ax.set_title('åŸå§‹è¨Šè™Ÿ vs. æ¿¾æ³¢å¾Œè¨Šè™Ÿ')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # é¡¯ç¤ºä¿¡è™Ÿçµ±è¨ˆä¿¡æ¯\n",
        "    print(f\"åŸå§‹ä¿¡è™Ÿæ¨™æº–å·®: {np.std(raw_mag_signal):.4f}\")\n",
        "    print(f\"æ¿¾æ³¢å¾Œä¿¡è™Ÿæ¨™æº–å·®: {np.std(mag_filtered):.4f}\")\n",
        "    print(f\"ä¿¡è™Ÿé•·åº¦: {len(mag_filtered)} å€‹å–æ¨£é»\")\n",
        "else:\n",
        "    print(\"âŒ æ²’æœ‰å¯ç”¨çš„åŸå§‹ä¿¡è™Ÿæ•¸æ“š\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å€å¡Š 4ï¼šè¨Šè™Ÿåˆ†æ®µ (Segmentation) èˆ‡ æ¨£æ¿å»ºç«‹ (Template Creation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================= #\n",
        "# å€å¡Š 4: è¨Šè™Ÿåˆ†æ®µ (Segmentation) èˆ‡ æ¨£æ¿å»ºç«‹ (Template Creation)\n",
        "# ================================================================= #\n",
        "\n",
        "# æˆ‘å€‘é¸æ“‡æ¿¾æ³¢å¾Œçš„è¨Šè™Ÿä¾†é€²è¡Œå¾ŒçºŒåˆ†æ\n",
        "if 'mag_filtered' in locals():\n",
        "    final_signal = mag_filtered\n",
        "    \n",
        "    print(\"æ­£åœ¨å°ä¹¾æ·¨çš„è¨Šè™Ÿé€²è¡Œè‡ªå‹•åˆ†æ®µ...\")\n",
        "    \n",
        "    # ä½¿ç”¨è‡ªé©æ‡‰åˆ†æ®µæ–¹æ³•\n",
        "    from nearpy.preprocess.segment import get_adaptive_segment_indices\n",
        "    \n",
        "    # ä½¿ç”¨ç§»å‹•æ¨™æº–å·® (Movstd) ä½œç‚ºåˆ¤æ–·ä¾æ“šï¼Œè‡ªå‹•æ‰¾å‡ºå‹•ä½œå€æ®µ\n",
        "    segments_indices = get_adaptive_segment_indices(\n",
        "        final_signal, \n",
        "        timeAx=np.linspace(0, len(final_signal)/fs, len(final_signal)),\n",
        "        fs=fs,\n",
        "        method='Movstd', \n",
        "        win_size=int(fs * 0.5),  # 0.5 ç§’çš„çª—æ ¼\n",
        "        prob_thresh=0.8  # é–¾å€¼\n",
        "    )\n",
        "    \n",
        "    if len(segments_indices) > 0:\n",
        "        num_segments = len(segments_indices)\n",
        "        print(f\"âœ… åœ¨è¨Šè™Ÿä¸­è‡ªå‹•åµæ¸¬åˆ° {num_segments} å€‹å‹•ä½œå€æ®µã€‚\")\n",
        "        \n",
        "        # å°‡æ‰€æœ‰åµæ¸¬åˆ°çš„å€æ®µç¹ªè£½å‡ºä¾†ï¼Œæª¢æŸ¥åˆ†æ®µæ•ˆæœ\n",
        "        print(\"æ­£åœ¨ç¹ªè£½åˆ†æ®µçµæœ...\")\n",
        "        fig, ax = plt.subplots(figsize=(15, 6))\n",
        "        \n",
        "        # ç¹ªè£½åŸå§‹ä¿¡è™Ÿ\n",
        "        ax.plot(final_signal, label='Filtered Signal', alpha=0.7)\n",
        "        \n",
        "        # æ¨™è¨˜åˆ†æ®µå€åŸŸ\n",
        "        for i, (start, end) in enumerate(segments_indices):\n",
        "            ax.axvspan(start, end, alpha=0.3, color=f'C{i}', \n",
        "                      label=f'Segment {i+1}' if i < 5 else None)\n",
        "        \n",
        "        ax.set_title(\"è‡ªå‹•åˆ†æ®µçµæœ (Detected Segments)\")\n",
        "        ax.set_xlabel(\"å–æ¨£é» (Sampling Points)\")\n",
        "        ax.set_ylabel(\"æŒ¯å¹… (Amplitude)\")\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # å»ºç«‹å‹•ä½œæ¨£æ¿ (Template)\n",
        "        print(\"\\næ­£åœ¨å»ºç«‹å‹•ä½œæ¨£æ¿...\")\n",
        "        \n",
        "        # 1. æ ¹æ“šç´¢å¼•ï¼Œå¾è¨Šè™Ÿä¸­æŠŠæ¯ä¸€æ®µå‹•ä½œçš„æ³¢å½¢åˆ‡å‡ºä¾†\n",
        "        signal_segments = []\n",
        "        for start, end in segments_indices:\n",
        "            segment = final_signal[start:end]\n",
        "            if len(segment) > 10:  # åªä¿ç•™é•·åº¦è¶³å¤ çš„æ®µ\n",
        "                signal_segments.append(segment)\n",
        "        \n",
        "        if len(signal_segments) > 0:\n",
        "            print(f\"æœ‰æ•ˆåˆ†æ®µæ•¸é‡: {len(signal_segments)}\")\n",
        "            \n",
        "            # 2. ç”±æ–¼æ¯æ®µé•·åº¦å¯èƒ½ä¸åŒï¼Œå°‡å®ƒå€‘è£œé›¶åˆ°ä¸€æ¨£é•·\n",
        "            max_len = max(len(s) for s in signal_segments)\n",
        "            padded_segments = np.array([np.pad(s, (0, max_len - len(s))) for s in signal_segments])\n",
        "            \n",
        "            # 3. ç°¡å–®å¹³å‡å»ºç«‹æ¨£æ¿ï¼ˆç°¡åŒ–ç‰ˆï¼Œå¯¦éš›ä¸­å¯ä»¥ä½¿ç”¨DTWï¼‰\n",
        "            template = np.mean(padded_segments, axis=0)\n",
        "            \n",
        "            print(\"âœ… æ¨£æ¿å»ºç«‹å®Œæˆï¼\")\n",
        "\n",
        "            # ç¹ªè£½æ¨£æ¿å’Œå€‹åˆ¥åˆ†æ®µ\n",
        "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "            \n",
        "            # ä¸Šåœ–ï¼šæ‰€æœ‰åˆ†æ®µ\n",
        "            for i, seg in enumerate(padded_segments[:5]):  # åªé¡¯ç¤ºå‰5å€‹\n",
        "                ax1.plot(seg, alpha=0.6, label=f'Segment {i+1}')\n",
        "            ax1.set_title(\"å€‹åˆ¥å‹•ä½œåˆ†æ®µ\")\n",
        "            ax1.set_ylabel(\"æŒ¯å¹…\")\n",
        "            ax1.legend()\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "            \n",
        "            # ä¸‹åœ–ï¼šå¹³å‡æ¨£æ¿\n",
        "            ax2.plot(template, color='red', linewidth=2, label='å‹•ä½œæ¨£æ¿ (Template)')\n",
        "            ax2.set_title(\"Tiptoe å‹•ä½œçš„å¹³å‡æ¨£æ¿\")\n",
        "            ax2.set_xlabel(\"å°é½Šå¾Œçš„å–æ¨£é» (Aligned Sampling Points)\")\n",
        "            ax2.set_ylabel(\"æ¨™æº–åŒ–æŒ¯å¹…\")\n",
        "            ax2.legend()\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            # ä¿å­˜æ¨£æ¿ä¾›å¾ŒçºŒä½¿ç”¨\n",
        "            template_signal = template\n",
        "        else:\n",
        "            print(\"âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åˆ†æ®µ\")\n",
        "    else:\n",
        "        print(\"âŒ æ²’æœ‰åµæ¸¬åˆ°ä»»ä½•å‹•ä½œå€æ®µ\")\n",
        "        # å¦‚æœè‡ªå‹•åˆ†æ®µå¤±æ•—ï¼Œæ‰‹å‹•å‰µå»ºä¸€å€‹ç°¡å–®çš„æ¨£æ¿\n",
        "        print(\"ä½¿ç”¨æ‰‹å‹•åˆ†æ®µæ–¹æ³•...\")\n",
        "        # å°‡ä¿¡è™Ÿåˆ†æˆ3æ®µä½œç‚ºç¤ºä¾‹\n",
        "        segment_len = len(final_signal) // 3\n",
        "        segments_indices = [(i*segment_len, (i+1)*segment_len) for i in range(3)]\n",
        "        \n",
        "        signal_segments = [final_signal[start:end] for start, end in segments_indices]\n",
        "        max_len = max(len(s) for s in signal_segments)\n",
        "        padded_segments = np.array([np.pad(s, (0, max_len - len(s))) for s in signal_segments])\n",
        "        template_signal = np.mean(padded_segments, axis=0)\n",
        "        \n",
        "        print(\"âœ… æ‰‹å‹•æ¨£æ¿å»ºç«‹å®Œæˆï¼\")\n",
        "else:\n",
        "    print(\"âŒ æ²’æœ‰å¯ç”¨çš„æ¿¾æ³¢å¾Œä¿¡è™Ÿ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================= #\n",
        "# è£œå……ï¼šå»ºç«‹å¯é‡ç”¨çš„ç¹ªåœ–å·¥å…·ï¼Œè¼¸å‡ºèˆ‡åƒè€ƒåœ–ç›¸è¿‘çš„è¦–åœ–\n",
        "# ================================================================= #\n",
        "\n",
        "from typing import List, Tuple, Optional\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "\n",
        "\n",
        "def plot_amp_phase_pairs(\n",
        "    data: pd.DataFrame,\n",
        "    pairs: List[Tuple[str, str]],\n",
        "    fs: float,\n",
        "    title_prefix: str = '',\n",
        "    t_window: Optional[Tuple[float, float]] = None,\n",
        "    downsample: int = 1,\n",
        "    center: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    å°‡å¤šçµ„ (Amp, Phase) ä»¥å †ç–Šå­åœ–æ–¹å¼ç¹ªè£½ï¼Œx è»¸ç‚ºç§’ï¼Œæ¨£å¼è²¼è¿‘åƒè€ƒåœ–ã€‚\n",
        "\n",
        "    - pairs: [(amp_col, phs_col), ...]\n",
        "    - t_window: (t_start, t_end) è‹¥æä¾›ï¼Œåªå–è©²æ™‚é–“çª—å£çš„è³‡æ–™\n",
        "    - downsample: ç¹ªåœ–å–æ¨£æ­¥å¹…ï¼ˆ>=1ï¼‰\n",
        "    - center: æ˜¯å¦ä»¥ä¸­ä½æ•¸ä¸­å¿ƒåŒ–æŒ¯å¹…èˆ‡ç›¸ä½\n",
        "    \"\"\"\n",
        "    n = len(pairs)\n",
        "    if n == 0:\n",
        "        print('No pairs to plot')\n",
        "        return\n",
        "\n",
        "    n_samples = len(data)\n",
        "    idx = np.arange(n_samples)\n",
        "\n",
        "    if t_window is not None:\n",
        "        t0, t1 = t_window\n",
        "        i0 = max(0, int(np.floor(t0 * fs)))\n",
        "        i1 = min(n_samples, int(np.ceil(t1 * fs)))\n",
        "        idx = np.arange(i0, i1)\n",
        "\n",
        "    if downsample > 1:\n",
        "        idx = idx[::downsample]\n",
        "\n",
        "    t = idx / fs\n",
        "\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(15, 2.3 * n), sharex=True)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, (amp_col, phs_col) in enumerate(pairs):\n",
        "        ax = axes[i]\n",
        "        amp = data[amp_col].to_numpy()\n",
        "        phs = data[phs_col].to_numpy()\n",
        "        if center:\n",
        "            amp = amp - np.nanmedian(amp)\n",
        "            phs = phs - np.nanmedian(phs)\n",
        "\n",
        "        ax.plot(t, amp[idx], color='tab:blue', lw=1.0, alpha=0.9, label='Amp')\n",
        "        ax.set_ylabel('Amp')\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, phs[idx], color='k', lw=1.0, alpha=0.8, label='Phs (rad)')\n",
        "        ax2.set_ylabel('Phs (rad)')\n",
        "        ax.set_title(f\"{title_prefix} Pair {i+1}  {amp_col}/{phs_col}\")\n",
        "        # æ¸…æ¥šçš„åˆ»åº¦\n",
        "        ax.xaxis.set_major_locator(MultipleLocator(0.5))\n",
        "        ax.xaxis.set_minor_locator(MultipleLocator(0.1))\n",
        "        ax.grid(True, which='major', alpha=0.35)\n",
        "        ax.grid(True, which='minor', alpha=0.15)\n",
        "\n",
        "    axes[-1].set_xlabel('æ™‚é–“ (s)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# å¿«é€Ÿå»ºç«‹æœ¬è³‡æ–™çš„é…å°ï¼ˆæ²¿ç”¨ä¸Šä¸€å–®å…ƒé‚è¼¯ï¼‰\n",
        "_cols = list(rf_data.columns)\n",
        "_pairs = []\n",
        "for i in range(0, len(_cols) - 1, 2):\n",
        "    _pairs.append((_cols[i], _cols[i+1]))\n",
        "\n",
        "# ç¯„ä¾‹ 1ï¼šé‡ç¾ã€ŒRawã€é¢¨æ ¼ï¼ˆæ™‚é–“çª— 5â€“11 sï¼‰\n",
        "plot_amp_phase_pairs(\n",
        "    rf_data, _pairs[:4], fs, title_prefix='Raw_', t_window=(5.0, 11.0), downsample=5, center=True\n",
        ")\n",
        "\n",
        "# ç¯„ä¾‹ 2ï¼šé‡ç¾ã€ŒFilteredã€é¢¨æ ¼ï¼šå°æ¯å€‹ amp åšè¼•åº¦å¹³æ»‘èˆ‡ä½é€š\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "rf_data_filt = rf_data.copy()\n",
        "for a, p in _pairs[:4]:\n",
        "    sig = rf_data_filt[a].to_numpy()\n",
        "    # æº«å’Œçš„å¹³æ»‘ï¼ˆä¸æ”¹è®Šå½¢ç‹€å¤ªå¤šï¼‰\n",
        "    win = max(5, int(0.15 * fs) // 2 * 2 + 1)\n",
        "    try:\n",
        "        rf_data_filt[a] = savgol_filter(sig, window_length=win, polyorder=2, mode='interp')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "plot_amp_phase_pairs(\n",
        "    rf_data_filt, _pairs[:4], fs, title_prefix='Filtered_', t_window=(5.0, 11.0), downsample=5, center=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å€å¡Š 5ï¼šç‰¹å¾µæå– (Feature Extraction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================= #\n",
        "# å€å¡Š 5: ç‰¹å¾µæå– (Feature Extraction)\n",
        "# ================================================================= #\n",
        "\n",
        "if 'template_signal' in locals():\n",
        "    print(\"æ­£åœ¨å¾æ¨£æ¿ä¿¡è™Ÿä¸­æå–ç‰¹å¾µ...\")\n",
        "    \n",
        "    # æå–æ™‚åŸŸç‰¹å¾µ\n",
        "    from nearpy.features.temporal import get_temporal_feats\n",
        "    \n",
        "    temporal_features = get_temporal_feats(template_signal)\n",
        "    \n",
        "    print(\"âœ… æ™‚åŸŸç‰¹å¾µæå–å®Œæˆï¼\")\n",
        "    print(\"æå–çš„ç‰¹å¾µ:\")\n",
        "    for feat_name, feat_value in temporal_features.items():\n",
        "        print(f\"  {feat_name}: {feat_value:.4f}\")\n",
        "    \n",
        "    # æå–é »åŸŸç‰¹å¾µ\n",
        "    from nearpy.preprocess.spectrum import ncs_fft\n",
        "    \n",
        "    # è¨ˆç®—é »è­œ\n",
        "    frequencies, power_spectrum = ncs_fft(template_signal, fs=fs)\n",
        "    \n",
        "    # è¨ˆç®—ä¸€äº›é »åŸŸç‰¹å¾µ\n",
        "    dominant_freq = frequencies[np.argmax(power_spectrum)]\n",
        "    spectral_energy = np.sum(power_spectrum)\n",
        "    spectral_centroid = np.sum(frequencies * power_spectrum) / np.sum(power_spectrum)\n",
        "    \n",
        "    spectral_features = {\n",
        "        'Dominant_Frequency': dominant_freq,\n",
        "        'Spectral_Energy': spectral_energy,\n",
        "        'Spectral_Centroid': spectral_centroid,\n",
        "        'Peak_Power': np.max(power_spectrum)\n",
        "    }\n",
        "    \n",
        "    print(\"\\nâœ… é »åŸŸç‰¹å¾µæå–å®Œæˆï¼\")\n",
        "    print(\"é »åŸŸç‰¹å¾µ:\")\n",
        "    for feat_name, feat_value in spectral_features.items():\n",
        "        print(f\"  {feat_name}: {feat_value:.4f}\")\n",
        "    \n",
        "    # ç¹ªè£½é »è­œåœ–\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # æ™‚åŸŸä¿¡è™Ÿå’Œç‰¹å¾µ\n",
        "    ax1.plot(template_signal, label='Template Signal')\n",
        "    ax1.set_title('å‹•ä½œæ¨£æ¿ (æ™‚åŸŸ)')\n",
        "    ax1.set_xlabel('å–æ¨£é»')\n",
        "    ax1.set_ylabel('æŒ¯å¹…')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend()\n",
        "    \n",
        "    # é »åŸŸä¿¡è™Ÿ\n",
        "    ax2.plot(frequencies, power_spectrum)\n",
        "    ax2.axvline(dominant_freq, color='red', linestyle='--', label=f'Dominant Freq: {dominant_freq:.2f} Hz')\n",
        "    ax2.set_title('åŠŸç‡é »è­œ')\n",
        "    ax2.set_xlabel('é »ç‡ (Hz)')\n",
        "    ax2.set_ylabel('åŠŸç‡')\n",
        "    ax2.set_xlim(0, 20)  # åªé¡¯ç¤º0-20Hzç¯„åœ\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # åˆä½µæ‰€æœ‰ç‰¹å¾µ\n",
        "    all_features = {**temporal_features, **spectral_features}\n",
        "    \n",
        "    # å‰µå»ºç‰¹å¾µDataFrame\n",
        "    features_df = pd.DataFrame([all_features])\n",
        "    \n",
        "    print(\"\\nâœ… æ‰€æœ‰ç‰¹å¾µæå–å®Œæˆï¼\")\n",
        "    print(\"\\nç‰¹å¾µæ‘˜è¦:\")\n",
        "    print(features_df)\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ æ²’æœ‰å¯ç”¨çš„æ¨£æ¿ä¿¡è™Ÿ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å€å¡Š 6ï¼šæ•¸æ“šé›†å‰µå»ºèˆ‡æ©Ÿå™¨å­¸ç¿’æº–å‚™\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================= #\n",
        "# å€å¡Š 6: æ•¸æ“šé›†å‰µå»ºèˆ‡æ©Ÿå™¨å­¸ç¿’æº–å‚™\n",
        "# ================================================================= #\n",
        "\n",
        "print(\"æ­£åœ¨æº–å‚™æ©Ÿå™¨å­¸ç¿’æ•¸æ“šé›†...\")\n",
        "\n",
        "# å‰µå»ºå¤šå€‹å‹•ä½œé¡åˆ¥çš„æ¨¡æ“¬æ•¸æ“šé›†\n",
        "def create_synthetic_dataset():\n",
        "    \"\"\"å‰µå»ºæ¨¡æ“¬çš„åå’½éšœç¤™æª¢æ¸¬æ•¸æ“šé›†\"\"\"\n",
        "    \n",
        "    # å®šç¾©å‹•ä½œé¡å‹\n",
        "    gestures = {\n",
        "        'rest': 0,      # ä¼‘æ¯\n",
        "        'swallow': 1,   # åå’½\n",
        "        'tiptoe': 2     # è¸®è…³\n",
        "    }\n",
        "    \n",
        "    # åƒæ•¸è¨­å®š\n",
        "    fs = 100\n",
        "    duration = 3  # æ¯å€‹å‹•ä½œ3ç§’\n",
        "    num_subjects = 3\n",
        "    num_reps = 5  # æ¯å€‹å‹•ä½œé‡è¤‡5æ¬¡\n",
        "    \n",
        "    dataset = []\n",
        "    \n",
        "    for subject in range(1, num_subjects + 1):\n",
        "        for gesture_name, gesture_id in gestures.items():\n",
        "            for rep in range(1, num_reps + 1):\n",
        "                # ç”Ÿæˆæ¨¡æ“¬ä¿¡è™Ÿ\n",
        "                t = np.linspace(0, duration, int(fs * duration))\n",
        "                \n",
        "                if gesture_name == 'rest':\n",
        "                    # ä¼‘æ¯ï¼šä½é »å°å¹…æ³¢å‹•\n",
        "                    signal = 1.0 + 0.1 * np.sin(2 * np.pi * 0.2 * t) + 0.05 * np.random.randn(len(t))\n",
        "                elif gesture_name == 'swallow':\n",
        "                    # åå’½ï¼šä¸­é »è¼ƒå¤§æ³¢å‹•\n",
        "                    signal = 1.0 + 0.4 * np.sin(2 * np.pi * 1.5 * t) + 0.1 * np.random.randn(len(t))\n",
        "                else:  # tiptoe\n",
        "                    # è¸®è…³ï¼šé«˜é »å¤§å¹…æ³¢å‹•\n",
        "                    signal = 1.0 + 0.6 * np.sin(2 * np.pi * 3.0 * t) + 0.15 * np.random.randn(len(t))\n",
        "                \n",
        "                # æ‡‰ç”¨æ¿¾æ³¢\n",
        "                filter_taps = get_gesture_filter(f_s=15, fs=fs, visualize=False)\n",
        "                filtered_signal = filtfilt(filter_taps, 1, signal)\n",
        "                \n",
        "                # æå–ç‰¹å¾µ\n",
        "                temporal_feats = get_temporal_feats(filtered_signal)\n",
        "                frequencies, power_spectrum = ncs_fft(filtered_signal, fs=fs)\n",
        "                \n",
        "                # ç°¡åŒ–ç‰¹å¾µé›†\n",
        "                features = {\n",
        "                    'Mobility': temporal_feats['Mobility'],\n",
        "                    'Complexity': temporal_feats['Complexity'],\n",
        "                    'Energy': temporal_feats['Energy'],\n",
        "                    'Dominant_Freq': frequencies[np.argmax(power_spectrum)],\n",
        "                    'Spectral_Energy': np.sum(power_spectrum),\n",
        "                    'Subject': subject,\n",
        "                    'Gesture': gesture_name,\n",
        "                    'Gesture_ID': gesture_id,\n",
        "                    'Repetition': rep\n",
        "                }\n",
        "                \n",
        "                dataset.append(features)\n",
        "    \n",
        "    return pd.DataFrame(dataset)\n",
        "\n",
        "# å‰µå»ºæ•¸æ“šé›†\n",
        "ml_dataset = create_synthetic_dataset()\n",
        "\n",
        "print(f\"âœ… æ•¸æ“šé›†å‰µå»ºå®Œæˆï¼\")\n",
        "print(f\"æ•¸æ“šé›†å½¢ç‹€: {ml_dataset.shape}\")\n",
        "print(f\"åŒ…å«çš„å‹•ä½œé¡åˆ¥: {ml_dataset['Gesture'].unique()}\")\n",
        "print(f\"åŒ…å«çš„å—è©¦è€…: {ml_dataset['Subject'].unique()}\")\n",
        "\n",
        "# é¡¯ç¤ºæ•¸æ“šé›†å‰å¹¾è¡Œ\n",
        "print(\"\\næ•¸æ“šé›†å‰5è¡Œ:\")\n",
        "print(ml_dataset.head())\n",
        "\n",
        "# çµ±è¨ˆä¿¡æ¯\n",
        "print(\"\\nå„å‹•ä½œé¡åˆ¥çš„æ¨£æœ¬æ•¸é‡:\")\n",
        "print(ml_dataset['Gesture'].value_counts())\n",
        "\n",
        "# å¯è¦–åŒ–ç‰¹å¾µåˆ†å¸ƒ\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "feature_cols = ['Mobility', 'Complexity', 'Energy', 'Dominant_Freq']\n",
        "\n",
        "for i, feature in enumerate(feature_cols):\n",
        "    for gesture in ml_dataset['Gesture'].unique():\n",
        "        data = ml_dataset[ml_dataset['Gesture'] == gesture][feature]\n",
        "        axes[i].hist(data, alpha=0.6, label=gesture, bins=10)\n",
        "    \n",
        "    axes[i].set_title(f'{feature} åˆ†å¸ƒ')\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel('é »ç‡')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å€å¡Š 7ï¼šç°¡å–®åˆ†é¡å™¨æ¼”ç¤º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================= #\n",
        "# å€å¡Š 7: ç°¡å–®åˆ†é¡å™¨æ¼”ç¤º\n",
        "# ================================================================= #\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"æ­£åœ¨è¨“ç·´ç°¡å–®çš„åˆ†é¡å™¨...\")\n",
        "\n",
        "# æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤\n",
        "feature_cols = ['Mobility', 'Complexity', 'Energy', 'Dominant_Freq', 'Spectral_Energy']\n",
        "X = ml_dataset[feature_cols]\n",
        "y = ml_dataset['Gesture_ID']\n",
        "\n",
        "# æ¨™æº–åŒ–ç‰¹å¾µ\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# åˆ†å‰²æ•¸æ“šé›†\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"è¨“ç·´é›†å¤§å°: {X_train.shape}\")\n",
        "print(f\"æ¸¬è©¦é›†å¤§å°: {X_test.shape}\")\n",
        "\n",
        "# è¨“ç·´éš¨æ©Ÿæ£®æ—åˆ†é¡å™¨\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100, \n",
        "    random_state=42, \n",
        "    max_depth=5\n",
        ")\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# é æ¸¬\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# è©•ä¼°\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nâœ… åˆ†é¡å™¨è¨“ç·´å®Œæˆï¼\")\n",
        "print(f\"æ¸¬è©¦æº–ç¢ºç‡: {accuracy:.3f}\")\n",
        "\n",
        "# è©³ç´°åˆ†é¡å ±å‘Š\n",
        "gesture_names = ['rest', 'swallow', 'tiptoe']\n",
        "print(\"\\nåˆ†é¡å ±å‘Š:\")\n",
        "print(classification_report(y_test, y_pred, target_names=gesture_names))\n",
        "\n",
        "# æ··æ·†çŸ©é™£\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# ç¹ªè£½æ··æ·†çŸ©é™£\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=gesture_names, yticklabels=gesture_names)\n",
        "plt.title('æ··æ·†çŸ©é™£ (Confusion Matrix)')\n",
        "plt.xlabel('é æ¸¬æ¨™ç±¤')\n",
        "plt.ylabel('çœŸå¯¦æ¨™ç±¤')\n",
        "plt.show()\n",
        "\n",
        "# ç‰¹å¾µé‡è¦æ€§\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': rf_classifier.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nç‰¹å¾µé‡è¦æ€§:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# ç¹ªè£½ç‰¹å¾µé‡è¦æ€§\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=feature_importance, x='Importance', y='Feature')\n",
        "plt.title('ç‰¹å¾µé‡è¦æ€§ (Feature Importance)')\n",
        "plt.xlabel('é‡è¦æ€§')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ‰ å®Œæ•´ä¿¡è™Ÿè™•ç†æµç¨‹æ¼”ç¤ºå®Œæˆï¼\")\n",
        "print(\"\\nç¸½çµ:\")\n",
        "print(\"1. âœ… æˆåŠŸè®€å–å’Œå¯è¦–åŒ–åŸå§‹RFä¿¡è™Ÿ\")\n",
        "print(\"2. âœ… æ‡‰ç”¨å¸¶é€šæ¿¾æ³¢å™¨å»é™¤å™ªè²\")\n",
        "print(\"3. âœ… é€²è¡Œä¿¡è™Ÿåˆ†æ®µå’Œæ¨£æ¿å»ºç«‹\")\n",
        "print(\"4. âœ… æå–æ™‚åŸŸå’Œé »åŸŸç‰¹å¾µ\")\n",
        "print(\"5. âœ… å‰µå»ºæ©Ÿå™¨å­¸ç¿’æ•¸æ“šé›†\")\n",
        "print(\"6. âœ… è¨“ç·´åˆ†é¡å™¨ä¸¦è©•ä¼°æ€§èƒ½\")\n",
        "print(\"\\né€™å€‹æµç¨‹ç‚ºåå’½éšœç¤™æª¢æ¸¬æä¾›äº†å®Œæ•´çš„ä¿¡è™Ÿè™•ç†åŸºç¤ï¼\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NFS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
